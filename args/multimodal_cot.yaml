# Multimodal CoT Pre-training Configuration (Stage 0)
# This configuration is for the foundation stage of multimodal CoCoNuT training

# Experiment settings
name: "multimodal-cot-aokvqa"
seed: 42

# Model configuration
model_id: "OpenGVLab/InternVL3-1B-Pretrained"
load_model_path: "None"  # No pre-trained model for Stage 0

# Training mode (Stage 0 = CoT pre-training)
cot: true
coconut: false
no_cot: false

# CoCoNuT parameters (not used in Stage 0 but needed for compatibility)
c_thought: 2
max_latent_stage: 4
epochs_per_stage: 5
uniform_prob: 0.0  # No mixing in Stage 0

# Training parameters
num_epochs: 20
batch_size_training: 4
batch_size_eval: 8
learning_rate: 1e-5
weight_decay: 0.01
warmup_steps: 500
gradient_accumulation_steps: 4
max_grad_norm: 1.0

# Multimodal parameters
image_size: 448
max_num_patches: 12
use_thumbnail: true
dynamic_preprocess: true

# Data paths (update these for your setup)
train_data_path: "data/aokvqa/train.json"
val_data_path: "data/aokvqa/val.json"
test_data_path: "data/aokvqa/test.json"
image_root: "data/aokvqa/images"

# Data limits (for testing/debugging)
max_train_samples: 1000000000  # No limit
max_val_samples: 1000000000    # No limit

# Distributed training
use_fsdp: true
use_ddp: false
num_workers: 4

# Evaluation
only_eval: false
eval_every_n_epochs: 2

# Checkpointing
save_path: "checkpoints"
resume: 0
save_every_n_epochs: 2
save_only_improve: false

# Logging
project: "multimodal-coconut"
debug: false

# Additional parameters
reset_optimizer: false
bf16: false
pad_latent_to_max: false